{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Numerical libraries\n",
    "import numpy as np   \n",
    "# to handle data in form of rows and columns \n",
    "import pandas as pd    \n",
    "\n",
    "# Import Linear Regression machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# importing ploting libraries\n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "#importing seaborn for statistical plots\n",
    "import seaborn as sns, sns.set()\n",
    "from scipy.stats import zscore\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ride and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for scaling\n",
    "\n",
    "# scale all the columns of the mpg_df. This will produce a numpy array\n",
    "X_scaled = preprocessing.scale(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)  # ideally the training and test should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fit a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_train, y_train)\n",
    "\n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a regularized RIDGE model and note the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=.3)\n",
    "ridge.fit(X_train,y_train)\n",
    "print (\"Ridge model:\", (ridge.coef_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a regularized LASSO model and note the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train,y_train)\n",
    "print (\"Lasso model:\", (lasso.coef_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us generate polynomial models reflecting the non-linear interaction between some dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.30, random_state=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fit a simple non regularized linear model on poly features-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.fit(X_train, y_train)\n",
    "print(regression_model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=.3)\n",
    "ridge.fit(X_train,y_train)\n",
    "print (\"Ridge model:\", (ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train,y_train)\n",
    "print (\"Lasso model:\", (lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of Ride and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT Ensemble Technique\n",
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion = 'entropy' , random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(train_set, train_labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT_With_Boundarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(DecisionTreeClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Regularising the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, random_state=1 )\n",
    "dt_model.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble Learning - Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bgcl = BaggingClassifier(n_estimators=100, max_samples=.50 , oob_score=True , random_state=1)\n",
    "\n",
    "bgcl = bgcl.fit(credit_df, credit_labels)\n",
    "print(bgcl.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble Learning - AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abcl = AdaBoostClassifier(base_estimator=dt_model, n_estimators=50, random_state=1)\n",
    "#abcl = AdaBoostClassifier(n_estimators=50)\n",
    "abcl = abcl.fit(train_set, train_labels)\n",
    "test_pred = abcl.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble Learning - GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbcl = GradientBoostingClassifier(n_estimators = 50, random_state=1)\n",
    "gbcl = gbcl.fit(train_set, train_labels)\n",
    "test_pred = gbcl.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfcl = RandomForestClassifier(n_estimators = 50, random_state=1)\n",
    "rfcl = rfcl.fit(train_set, train_labels)\n",
    "test_pred = rfcl.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of DT Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  stacking or mix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = pima_df.values\n",
    "X = array[:,0:7] \n",
    "Y = array[:,8]   \n",
    "test_size = 0.30 \n",
    "seed = 7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion = 'entropy', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcl = LogisticRegression(random_state=1)\n",
    "rfcl = RandomForestClassifier(random_state=1)\n",
    "nbcl = GaussianNB()\n",
    "bgcl = BaggingClassifier(base_estimator=dt_model, n_estimators=50 , random_state=1)  \n",
    "#the base_estimator can be null. The bagging classifer  will build it's own tree\n",
    "\n",
    "enclf = VotingClassifier(estimators = [('lor', lrcl), ('rf', rfcl), ('nb', nbcl), ('bg', bgcl)], voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, label in zip([lrcl , rfcl, nbcl, enclf, bgcl], ['Logistic Regression', 'RandomForest', 'NaiveBayes', 'Ensemble', 'Bagging']):\n",
    "    scores = cross_val_score(clf, X, Y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.02f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Support Vector Machine on train data\n",
    "SVC = SVC(C=.2, kernel='linear', gamma= 1)\n",
    "SVC = SVC.fit(X_train, y_train)\n",
    "prediction = SVC.predict(X_test)\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(prediction,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision_Tree_Regressor_Concrete_Regularization_GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## Iter 2... drop useless columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['FlyAsh' , 'CoarseAggregate' , 'FineAggregate' , 'Superplasticize' , 'CompStrength']\n",
    "\n",
    "X = concrete_df_z.drop(labels= drop_cols , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "#import pydotplus as pydot\n",
    "from sklearn import tree\n",
    "from os import system\n",
    "\n",
    "Credit_Tree_File = open('concrete_tree.dot','w')\n",
    "dot_data = tree.export_graphviz(dt_model, out_file=Credit_Tree_File, feature_names = list(X_train))\n",
    "\n",
    "Credit_Tree_File.close()\n",
    "\n",
    "system(\"dot -Tpng concrete_tree.dot -o concrete_tree.png\")\n",
    "Image(\"concrete_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## iteration 3 Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_range = range( 1, 15 )\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "  clusters = KMeans( num_clusters, n_init = 10 )\n",
    "  clusters.fit(concrete_df_z)\n",
    "  labels = clusters.labels_\n",
    "  centroids = clusters.cluster_centers_\n",
    "  cluster_errors.append( clusters.inertia_ )\n",
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n",
    "clusters_df[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans = KMeans(n_clusters= 5, random_state=2345)\n",
    "kmeans = KMeans(n_clusters= 6)\n",
    "kmeans.fit(concrete_df_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "counts = np.bincount(labels[labels>=0])\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a new dataframe only for labels and converting it into categorical variable\n",
    "cluster_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\n",
    "cluster_labels['labels'] = cluster_labels['labels'].astype('category')\n",
    "concrete_df_labeled = concrete_df.join(cluster_labels)\n",
    "\n",
    "concrete_df_labeled.boxplot(by = 'labels',  layout=(3,3), figsize=(30, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Iteration 4 ----- Ensemble Techniques...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbmTree = GradientBoostingRegressor(n_estimators=50)\n",
    "gbmTree.fit(X_train,y_train)\n",
    "print(\"gbmTree on training\" , gbmTree.score(X_train, y_train))\n",
    "print(\"gbmTree on test data \",gbmTree.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgcl = BaggingRegressor(n_estimators=50, oob_score= True)\n",
    "bgcl = bgcl.fit(X_train,y_train)\n",
    "print(\"bgcl on train data \", bgcl.score(X_train,y_train))\n",
    "print(\"bgcl on test data \", bgcl.score(X_test,y_test))\n",
    "print(\"out of bag score\" , bgcl.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfTree = RandomForestRegressor(n_estimators=50)\n",
    "rfTree.fit(X_train,y_train)\n",
    "print(\"rfTree on train data \", rfTree.score(X_train,y_train))\n",
    "print(\"rfTree on test data \", rfTree.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_XY = X.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "values = concrete_XY.values\n",
    "\n",
    "n_iterations = 1000        # Number of bootstrap samples to create\n",
    "n_size = int(len(concrete_df_z) * 1)    # size of a bootstrap sample\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()   # empty list that will hold the scores for each bootstrap iteration\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    # prepare train and test sets\n",
    "\ttrain = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "\ttest = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    \n",
    "    \n",
    "    # fit model\n",
    "\tgbmTree = GradientBoostingRegressor(n_estimators=50)\n",
    "\tgbmTree.fit(train[:,:-1], train[:,-1])   # fit against independent variables and corresponding target values\n",
    "\ty_test = test[:,-1]    # Take the target column for all rows in test set\n",
    "\n",
    "    # evaluate model\n",
    "\tpredictions = gbmTree.predict(test[:, :-1])   # predict based on independent variables in the test data\n",
    "\tscore = gbmTree.score(test[:, :-1] , y_test)\n",
    "\n",
    "\tstats.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "n_iterations = 1000              # Number of bootstrap samples to create\n",
    "n_size = int(len(concrete_df_z) * 1)    # size of a bootstrap sample\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()   # empty list that will hold the scores for each bootstrap iteration\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    # prepare train and test sets\n",
    "\ttrain = resample(values, n_samples=n_size)  # Sampling with replacement\n",
    "\ttest = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "\n",
    "    # fit model\n",
    "\trfTree = RandomForestRegressor(n_estimators=50)  \n",
    "\trfTree.fit(train[:,:-1], train[:,-1])   # fit against independent variables and corresponding target values\n",
    "\n",
    "\trfTree.fit(train[:,:-1], train[:,-1])   # fit against independent variables and corresponding target values\n",
    "\ty_test = test[:,-1]    # Take the target column for all rows in test set\n",
    "\n",
    "    # evaluate model\n",
    "\tpredictions = rfTree.predict(test[:, :-1])   # predict based on independent variables in the test data\n",
    "\tscore = rfTree.score(test[:, :-1] , y_test)\n",
    "\n",
    "\tstats.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Tuning using hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linspace(start = 5, stop = 10, num = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10 , stop = 15, num = 2)]   # returns evenly spaced 10 numbers\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 10, num = 2)]  # returns evenly spaced numbers can be changed to any\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 5, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "\n",
    "best_random.score(X_test , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearcCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5,6],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [5,10],\n",
    "    'n_estimators': [5,6,7]\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "model_score = model.score(X_test, y_test)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
